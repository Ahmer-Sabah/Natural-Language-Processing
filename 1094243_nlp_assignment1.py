# -*- coding: utf-8 -*-
"""1094243_NLP_Assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-0Jlhvg53tg1R8P8JRwTqBWzD9eBplsh
"""

#import libraries
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error,r2_score
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

dataset = pd.read_csv('/content/sample_data/housing.csv')
dataset = dataset.dropna() # drop missing data
print("Here are the first ten rows of the dataset:")
dataset.head(10)

# Data Visualization
dataset.plot(subplots='true')
plt.tight_layout()
plt.show()
#plt.savefig('plot.png')

Y = dataset['median_house_value'] #we'll predict the 'median_house_value' column

#The remaining columns i.e "longitude" to "median_income" will be used to predict Y 
X = dataset.loc[:,'longitude':'median_income']

# Split the dataset as 70%  for training and 30% for testing
x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=2003)

# Converts the training datasets to numpy arrays to work with our PyTorch model
x_train_np = x_train.to_numpy()
y_train_np = y_train.to_numpy()

# Converts the testing datasets to numpy arrays to work with our PyTorch model
x_test_np = x_test.to_numpy()
y_test_np = y_test.to_numpy()

#import libraries

import torch # pytorch library
from torch.nn import Conv1d # 1D convolution layer
from torch.nn import MaxPool1d # max pooling layer
from torch.nn import Flatten # flatten layer
from torch.nn import Linear #  linear layer
from torch.nn.functional import relu #  ReLU activation function
from torch.utils.data import DataLoader,TensorDataset #  DataLoader and TensorDataset libraries from PyTorch to work with our datasets

# Modular Programming
class CnnRegressor(torch.nn.Module): # Our class MUST be a subclass of torch.nn.Module
  def __init__(self,batch_size,inputs,outputs): # Define the initialization method
    super(CnnRegressor,self).__init__() # Initialize the superclass and store the parameters
    self.batch_size = batch_size
    self.inputs = inputs
    self.outputs = outputs

    self.input_layer = Conv1d(inputs,batch_size,1) # Define the input layer with parameters(input channels, output channels, kernel size)

    self.max_pooling_layer = MaxPool1d(1) # Define a max pooling layer(kernel size)

    #define convolution layers
    self.conv_layer = Conv1d(batch_size,128,1)
    self.conv_layer2 = Conv1d(128,64,1)
    #self.conv_layer3 = Conv1d(64,32,1)

    self.flatten_layer = Flatten() #define a flatten layer
   
    self.linear_layer = Linear(64,32) #define linear layer(inputs,outputs)
    
    self.output_layer = Linear(32,outputs) #finally,define the output layer

# Define a method to feed inputs through the model
  def feed(self,input):
    # Reshape the entry so it can be fed to the input layer
    # Although weâ€™re using 1D convolution, it still expects a 3D array to process in a 1D fashion
    input = input.reshape((self.batch_size,self.inputs,1))

    # Get the output of the first layer and run it through the
    # the ReLU activation function
    output = relu(self.input_layer(input)) 

    # Get the output of the max pooling layer
    output = self.max_pooling_layer(output) 

    # Get the output of the convolution layers and run it
    # through the ReLU activation function
    output = relu(self.conv_layer(output))
    output = relu(self.conv_layer2(output))
    #output = relu(self.conv_layer3(output))

    # Get the output of the flatten layer
    output = self.flatten_layer(output)
    
    # The output of the linear layer is run through the ReLU activation function
    output = self.linear_layer(output)

    # Finally, get the output of the output layer and return it
    output = self.output_layer(output)
    return output

# Import the SGD (stochastic gradient descent) package from pytorch for
# our optimizer
from torch.optim import SGD

# Import the ADAM package from pytorch for  our optimizer
from torch.optim import Adam

# Import the L1Loss (mean absolute error loss) package for our performance measure
from torch.nn import L1Loss

!pip install pytorch-ignite
from ignite.contrib.metrics.regression.r2_score import R2Score

# we define our model
# define the batch size we'd like to use
batch_size = 64
#batch_size = 32

#(batch size, X columns , Y columns)
model = CnnRegressor(batch_size,X.shape[1],1)

#Set the model to use the GPU for processing
model.cuda()

#create a method for running the batches of data through our model
# This method will return the average L1 loss and R^2 score
# of the passed model on the passed DataLoader
def model_loss(model,dataset,train = False,optimizer = None):
  # Cycle through the batches and get the average L1 loss
  performance = L1Loss()
  score_metric = R2Score()

  avg_loss = 0
  avg_score = 0
  count = 0
  for input,output in iter(dataset):
    # Get the model's predictions for the training dataset
    predictions = model.feed(input)

    # Get the model's loss
    loss = performance(predictions,output)

    # Get the model's R^2 score
    score_metric.update([predictions,output])
    score = score_metric.compute()

    if(train):
      # Clear any errors so they don't cummulate
      optimizer.zero_grad()
      # Compute the gradients for our optimizer
      loss.backward()

      #use the optimizer to update the model's paramters based on the gradients
      optimizer.step()

    # store the loss and update the counter
    avg_loss += loss.item()
    avg_score += score
    count += 1

  return avg_loss /count , avg_score / count

import time
epochs = 1000
#learning_rate = 0.007
#define the perfomance measure and optimizer
optimizer = SGD(model.parameters(), lr = 1e-5)
#optimizer = SGD(model.parameters(), lr = learning_rate)

#define the perfomance measure and optimizer
#optimizer = Adam(model.parameters(), lr = 1e-5)
#optimizer = Adam(model.parameters(), lr = learning_rate)

# Convert the training set into torch variables for our model using the GPU as floats. 
inputs = torch.from_numpy(x_train_np).cuda().float()
outputs = torch.from_numpy(y_train_np.reshape(y_train_np.shape[0],1)).cuda().float()

# Create a DataLoader instance to work with our batches
tensor = TensorDataset(inputs,outputs)
loader = DataLoader(tensor,batch_size,shuffle = True,drop_last=True)
begin_time = time.time()

# Start the training loop
for epoch in range(epochs):
  # Cycle through the batches and get the average loss
  avg_loss,avg_r2_score = model_loss(model,loader,train=True, optimizer=optimizer)
  # Output the average loss
  print("Epoch " +str(epoch + 1)+ ":\n\tLoss = " +str(avg_loss)+ "\n\tR^2 Score = " +str(avg_r2_score))

modelpath = '/content/sample_data/1094243_1dconv_reg.pth'
torch.save(model,modelpath)
model = torch.load(modelpath)

# Convert the testing set into torch variables for our model using the GPU  as floats
inputs = torch.from_numpy(x_test_np).cuda().float()
outputs = torch.from_numpy(y_test_np.reshape(y_test_np.shape[0],1)).cuda().float()

# Create a DataLoader instance to work with our batches
tensor = TensorDataset(inputs,outputs)
loader = DataLoader(tensor,batch_size,shuffle=True,drop_last=True)
end_time = time.time()
# Output the inference time of the model
print ("The model's inference time is: " +str(end_time-begin_time))
# Output the average performance of the model
avg_loss,avg_r2_score = model_loss(model,loader)
print("The model's L1 loss is: " +str(avg_loss))
print("The model's R^2 score is:" +str(avg_r2_score))